PROVISION AWS EMR WITH PYSPARK

Project Summary
In this project, we built a scalable data engineering pipeline to extract, transform, and load (ETL) Redfin real estate data using Amazon EMR and PySpark within a Jupyter notebook environment. The goal was to demonstrate how to provision and use AWS managed big data services to process large datasets and prepare them for downstream analytics or machine learning.
Technology	Purpose
Amazon EMR	Provisioned big data cluster to run distributed PySpark jobs
PySpark	Data processing, transformations, null handling
Jupyter Notebook (EMR Studio)	Interactive development environment for coding and visualization
AWS S3	Data storage for raw and transformed datasets (CSV and Parquet)
AWS CLI (wget)	Downloading source data directly into EMR cluster
Parquet Format	Efficient, columnar storage format for big data
	

Objective:

Provision a scalable big data processing environment, download raw Redfin real estate CSV data, and load it into a Spark DataFrame for analysis.

Technical Details:

•	Amazon EMR (Elastic MapReduce):
A managed cluster platform comes with preinstalled big data framework and simplifies running big data frameworks like Apache Spark, Hadoop, Apache Flink and Hive on AWS. EMR allows on-demand cluster provisioning with optimized compute and storage resources, providing scalable and distributed data processing.
•	EMR Studio & Jupyter Notebook:
Interactive environment provided by AWS EMR to write and run PySpark code in notebooks, making it easier to develop and debug large-scale data pipelines.
•	Spark:
An in-memory distributed computing engine optimized for fast data processing on large datasets.

Steps:

1.	Create IAM User
Next, create a new IAM user for this specific project. Ensure the user has AWS Management Console access and programmatic access by generating Access Key ID and Secret Access Key. These credentials will allow the user to interact with AWS services via AWS SDKs or CLI. Log in to the AWS Console using this new IAM user.

2.	Create Two S3 Buckets
•	One for raw Redfin data
•	One for transformed output data

3.	Create a VPC and Subnets
•	VPC and other networking resources
•	2 AZ with 2 public subnets
•	VPC endpoints: S3 Gateway 
•	Configure routing, gateway, and security group

4.	Provisioned an Amazon EMR Cluster

•	Selected EMR release with Apache Spark
•	Included Jupyter Enterprise Gateway
•	Primary and Core Node: m5.xlarge
•	Cluster scaling and provisioning: Use EMR-managed scaling
•	Minimum cluster size: 3
•	Maximum cluster size: 10
•	Maximum core nodes in the cluster: 8
•	Maximum On-Demand instances in the cluster: 8
•	Provisioning configuration instance: 2
•	Automatically terminate cluster after: 2 days
•	It will automatically create a bucket and store logs
•	Create EC2 Key Pair and service role with the previously create VPC and its default security group. The service role is an IAM role that Amazon EMR assumes to provision resources and perform service-level actions with other AWS services 
•	Create EC2 instance profile for Amazon EMR. The instance profile assigns a role to every EC2 instance in a cluster. The instance profile must specify a role that can access the resources for your steps and bootstrap actions. Let Amazon EMR create a new instance profile so that you can specify a custom set of resources for it to access in Amazon S3: All S3 buckets in this account with read and write access


