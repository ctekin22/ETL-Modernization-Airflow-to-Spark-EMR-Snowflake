CLOUD-NATIVE PYTHON-BASED ETL PIPELINE on AWS with AIRFLOW ORCHESTRATION and SNOWFLAKE 

In this project, the goal is to build an end-to-end data pipeline that extracts real estate data from the Redfin data source and processes it entirely on the AWS cloud. The pipeline begins by provisioning an EC2 instance using an Ubuntu AMI, where Apache Airflow is installed to orchestrate and automate the workflow. The raw data is first extracted from Redfin and stored in an Amazon S3 bucket. Using Python, the data is then cleaned and transformed before being saved to a separate S3 bucket. Once the transformed data lands in S3, it automatically triggers Snowpipe, a feature in Snowflake that loads the data into a Snowflake table in near real-time. Finally, Power BI is connected to the Snowflake database for data visualization.

1.	Extract real estate data from Redfin's public datasets.
2.	Load the raw data into an Amazon S3 bucket.
3.	Transform the data using Python and Pandas.
4.	Store the cleaned data back in S3 in Parquet format.
5.	Trigger Snowflake Snowpipe automatically once the transformed data lands in S3.
6.	Load the data into a Snowflake table.
7.	Visualize it in Power BI connected to Snowflake

1.	Building the Apache Airflow DAG and Implement Extraction & Transformation Tasks

We begin this step by defining the DAG configuration parameters and implementing the first two PythonOperator tasks in Airflow — one for extracting data from Redfin and another for transforming it.

Source:
https://www.redfin.com/news/data-center/ 

https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

Refer to scripts/dags/redfin_analytics.ipynb for implementation 

We'll specify the DAG's configuration, including:
•	Start date: datetime(2025, 10, 2)
•	Email on failure/retry: Set to False for now, but can be configured to send notifications
•	Schedule interval: Since Redfin updates its data monthly (typically during the third full week), we recommend scheduling the DAG to run weekly or monthly depending on use-case.

1.2.	 Extract Redfin Data Task- extract_redfin_data
We define a PythonOperator task called extract_redfin_data task, which downloads city-level Redfin housing data and saves it to the EC2 instance with a timestamp-based filename.

1.3.	 Transform the Data Task- transform_redfin_data
Once the raw data is extracted, we define a second PythonOperator transform_redfin_data task that reads the CSV, performs transformations using Pandas, and saves the cleaned data back into S3 in Parquet format.
Initial Data Exploration, Cleanup:
•	Count total rows to understand dataset size (approx. 60 million rows).
•	Detect and count null values in each column using PySpark functions.
•	Drop rows with any null values (dropna()), resulting in a cleaner dataset
•	Extract year and month from the period_end date column as new separate columns.
•	Drop the original period_end and last_updated columns to reduce redundancy.
•	Map numeric month values (1-12) to their corresponding month names (January, February, etc.) for better interpretability.


1.4.	 Load Raw Extracted Data to S3 using BashOperator Task - load_to_s3
The raw CSV file was saved locally in EC2 during the extraction step. We want to preserve the original dataset in an S3 bucket. This way, you have: Transformed data in one S3 bucket for analytics. Raw data in another for traceability or reprocessing.

•	Save the cleaned and transformed DataFrame as a Parquet file to a dedicated AWS S3 bucket.
•	Use overwrite mode to ensure that the output replaces any existing data.
•	Confirm via the AWS S3 Console that the Parquet file was successfully written.

2.	Load Transformed Redfin Data into Snowflake via Snowpipe (with Airflow Orchestration)
Goal is to automate the process of loading transformed CSV data from your S3 bucket into Snowflake every time Airflow drops a new file into S3, Snowflake ingests it without manual intervention.

Component	Role
S3	Stores your cleaned/transformed CSV files (Airflow uploads them here)
Snowflake	Data warehouse where you’ll query, analyze, and visualize data
Snowpipe	A Snowflake service that automatically detects and ingests new files from S3 into a target table
Airflow (on EC2)	Still orchestrating the ETL process — uploads transformed data to S3

•	Creating Snowflake resources: database, schema, and table
•	Configuring Snowpipe to auto-load data from S3
•	Connecting S3 event notifications to trigger Snowpipe
•	Testing the pipeline end-to-end
•	Visualizing Snowflake data with BI tools


