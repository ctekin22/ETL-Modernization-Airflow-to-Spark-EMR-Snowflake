CLOUD-NATIVE PYTHON-BASED ETL PIPELINE on AWS with AIRFLOW ORCHESTRATION and SNOWFLAKE 

In this project, the goal is to build an end-to-end data pipeline that extracts real estate data from the Redfin data source and processes it entirely on the AWS cloud. The pipeline begins by provisioning an EC2 instance using an Ubuntu AMI, where Apache Airflow is installed to orchestrate and automate the workflow. The raw data is first extracted from Redfin and stored in an Amazon S3 bucket. Using Python, the data is then cleaned and transformed before being saved to a separate S3 bucket. Once the transformed data lands in S3, it automatically triggers Snowpipe, a feature in Snowflake that loads the data into a Snowflake table in near real-time. Finally, Power BI is connected to the Snowflake database for data visualization.

1.	Extract real estate data from Redfin's public datasets.
2.	Load the raw data into an Amazon S3 bucket.
3.	Transform the data using Python and Pandas.
4.	Store the cleaned data back in S3 in Parquet format.
5.	Trigger Snowflake Snowpipe automatically once the transformed data lands in S3.
6.	Load the data into a Snowflake table.
7.	Visualize it in Power BI connected to Snowflake

1.	Building the Apache Airflow DAG and Implement Extraction & Transformation Tasks

We begin this step by defining the DAG configuration parameters and implementing the first two PythonOperator tasks in Airflow — one for extracting data from Redfin and another for transforming it.

Source:
https://www.redfin.com/news/data-center/ 

https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

Refer to scripts/dags/redfin_analytics.ipynb for implementation 

We'll specify the DAG's configuration, including:
•	Start date: datetime(2025, 10, 2)
•	Email on failure/retry: Set to False for now, but can be configured to send notifications
•	Schedule interval: Since Redfin updates its data monthly (typically during the third full week), we recommend scheduling the DAG to run weekly or monthly depending on use-case.

1.2.	 Extract Redfin Data Task- extract_redfin_data
We define a PythonOperator task called extract_redfin_data task, which downloads city-level Redfin housing data and saves it to the EC2 instance with a timestamp-based filename.

1.3.	 Transform the Data Task- transform_redfin_data
Once the raw data is extracted, we define a second PythonOperator transform_redfin_data task that reads the CSV, performs transformations using Pandas, and saves the cleaned data back into S3 in Parquet format.
Initial Data Exploration, Cleanup:
•	Count total rows to understand dataset size (approx. 60 million rows).
•	Detect and count null values in each column using PySpark functions.
•	Drop rows with any null values (dropna()), resulting in a cleaner dataset
•	Extract year and month from the period_end date column as new separate columns.
•	Drop the original period_end and last_updated columns to reduce redundancy.
•	Map numeric month values (1-12) to their corresponding month names (January, February, etc.) for better interpretability.


1.4.	 Load Raw Extracted Data to S3 using BashOperator Task - load_to_s3
The raw CSV file was saved locally in EC2 during the extraction step. We want to preserve the original dataset in an S3 bucket. This way, you have: Transformed data in one S3 bucket for analytics. Raw data in another for traceability or reprocessing.

•	Save the cleaned and transformed DataFrame as a Parquet file to a dedicated AWS S3 bucket.
•	Use overwrite mode to ensure that the output replaces any existing data.
•	Confirm via the AWS S3 Console that the Parquet file was successfully written.

2.	Load Transformed Redfin Data into Snowflake via Snowpipe (with Airflow Orchestration)
Goal is to automate the process of loading transformed CSV data from your S3 bucket into Snowflake every time Airflow drops a new file into S3, Snowflake ingests it without manual intervention.

Component	Role
S3	Stores your cleaned/transformed CSV files (Airflow uploads them here)
Snowflake	Data warehouse where you’ll query, analyze, and visualize data
Snowpipe	A Snowflake service that automatically detects and ingests new files from S3 into a target table
Airflow (on EC2)	Still orchestrating the ETL process — uploads transformed data to S3

•	Creating Snowflake resources: database, schema, and table
•	Configuring Snowpipe to auto-load data from S3
•	Connecting S3 event notifications to trigger Snowpipe
•	Testing the pipeline end-to-end
•	Visualizing Snowflake data with BI tools

2.1.	 Create Database, Schema, and Table in Snowflake
Snowflake needs to know where to put the data (database + schema + table). You can’t load CSVs without predefining the structure. Snowflake needs to understand the structure and rules of your files (CSV, JSON, etc.) to correctly parse and ingest them.

Snowflake needs compute power to execute queries. This is done using virtual warehouses.
•	Create your table matching the transformed CSV structure

Refer to scripts/snowflake/redfin_snowflake.sql for implementation

2.2.	 Create and Configure Snowpipe to Auto-Load from S3
Snowpipe is a service that: Watches a specified S3 location. When a new file appears, it automatically runs a COPY INTO command to insert the data into a table. Without Snowpipe, you'd need to manually run SQL to load data every time Airflow uploads a file. With Snowpipe, this becomes real-time and event-driven.

•	Create STAGE in Snowflake pointing to S3 bucket. A stage is a reference to an external data location. In this case, you want Snowflake to know:
o	Where to look (your S3 bucket)
o	How to authenticate (AWS keys or IAM role)
o	How the data is structured (via file format object)
o	Use/Create an IAM role in AWS that:
	Allows Snowflake to assume the role
	Grants read access to the specific S3 path (s3://redfin-transformed-data-bucket-airflow/)
•	Create PIPE (Snowpipe) to ingest data. Just like with file formats and stages, you keep Snowpipe definitions in their own schema. This separation is essential in larger pipelines with dozens of pipes. Manually running COPY INTO every time a file lands in S3 is error-prone and slow. Snowpipe automates this. When a new file is detected in the stage:
•	It reads the file
•	Uses the file format definition
•	Runs the COPY INTO command behind the scenes
•	Inserts rows into your target table within seconds or minutes
•	Snowpipe runs in micro-batches, not streaming
•	It's event-driven if you connect S3 notifications (see next step)
•	Can be triggered by manual call, S3 event notification, or REST API
•	You configure Snowpipe with:
•	The target table 
•	The source stage 
•	The file format 
Snowpipe works with Amazon SQS, which listens for S3 events. Snowflake generates an SQS-compatible ARN when the pipe is created.
•	Run DESCRIBE PIPE your_pipe_name
•	Extract the NOTIFICATION_CHANNEL value (an SQS ARN)
•	You'll use this when setting up the event in the S3 bucket

2.3.	  Set Up S3 Event Notifications to Trigger Snowpipe
You need to tell S3 to notify Snowflake (via SQS) whenever a new object is added to the bucket or folder where transformed data land.
•	Go to your S3 bucket’s Properties > Event Notification
•	Create a new event:
o	Name: e.g., snowpipe_trigger_redfin
o	Event Type: All object create events (PUT, POST, etc.)
o	Destination: SQS (paste the SQS ARN from DESCRIBE PIPE)
•	Snowpipe will then auto-ingest the file without delay.

2.4.	 Trigger Airflow DAG to Simulate Full Pipeline
As soon as upload completes:
•	S3 triggers the event
•	Event hits the SQS queue
•	Snowpipe is notified and runs
•	COPY INTO is executed automatically
•	Data appears in Snowflake table

PART 3: Same Pipeline with PySpark on Amazon EMR: tomate EMR Jobs with Airflow

3.	Provisioning EMR and Redfin Extraction
3.1. Create an S3 bucket (e.g., redfin-emr) and folders:
•	raw_data_ – raw Redfin data
•	transformed_data – transformed output
•	scripts – PySpark or shell scripts for EMR
•	emr-logs – store EMR logs

Start of the Pipeline
The workflow initiates with an EmptyOperator called start_pipeline. This does not perform any operations but serves as a logical beginning to visually represent the entry point in the Airflow UI. Such empty operators enhance DAG readability and structure, especially in complex workflows.

3.3.	 Creating an EMR Cluster
The next operation involves provisioning an EMR cluster using the EmrCreateJobFlowOperator. This operator launches a new cluster according to the configuration specified in job_flow_overrides. The next operation involves provisioning an EMR cluster using the EmrCreateJobFlowOperator. This operator launches a new cluster using the configurations defined in job_flow_overrides. These configurations include the following:
•	Cluster Name: redfin_emr_cluster
•	Release Label: emr-6.13.0
•	Applications Installed: Apache Spark and JupyterEnterpriseGateway
•	Logging: Enabled via LogUri pointing to an S3 bucket (s3://redfin-data-emr/emr-logs/)
•	Visibility: Set to False, so it's not visible to all AWS users by default
•	Cluster Composition: 1 master node and 2 core nodes, all of type m5.xlarge, running as on-demand instances
•	Network Settings: Deployed in subnet subnet-087d3438cd6469ba0 and uses the key pair airflow-key for SSH access
•	Lifecycle: KeepJobFlowAliveWhenNoSteps set to True allows us to add steps dynamically before auto-termination
•	IAM Roles: Uses standard roles EMR_EC2_DefaultRole for EC2 and EMR_DefaultRole for the cluster

This operation requires the apache-airflow-providers-amazon package. Without it, Airflow cannot recognize the EMR-related operators like EmrCreateJobFlowOperator, EmrAddStepsOperator, and EmrTerminateJobFlowOperator. This package provides official AWS integrations for Airflow, allowing orchestration of EMR, S3, Lambda, Redshift, and other AWS services. If this provider is not installed, a ModuleNotFoundError will be raised.
No need to specify aws_conn_id in the operator: AWS credentials are configured using IAM roles. This makes the DAG cleaner and avoids hardcoding credentials.
Upon execution, this operator returns a unique job_flow_id, which will be used to track and control the cluster through the rest of the DAG.
3.4.	 Checking Cluster Readiness
Once the EMR cluster is requested, its readiness is confirmed using the EmrJobFlowSensor. This sensor checks whether the EMR cluster has reached the WAITING state, meaning it's provisioned and idle, ready to execute Spark jobs. The sensor accesses the job_flow_id (key: return value, value:CLUSTER ID (same as AWS EMR)) from the output of the cluster creation task through Airflow's XCom mechanism, which supports cross-task data sharing. The cluster status is polled every 5 seconds (via poke_interval) until it is marked as ready. This prevents the pipeline from proceeding prematurely and ensures that resources are fully provisioned before Spark job submission begins.
Again, there is no need to set aws_conn_id.

Adding Extraction Step
After confirming that the EMR cluster is in a ready state, the DAG proceeds to submit a Spark job using the EmrAddStepsOperator. This operator is responsible for pushing executable steps (or jobs) to the EMR cluster. But to define what the cluster actually does, we use a Python object called SPARK_STEPS_EXTRACTION.


•	After ensuring the cluster is in a ready state, the DAG submits a Spark job using the EmrAddStepsOperator. 
•	This job adds an EMR step defined in the SPARK_STEPS_EXTRACTION object. The step leverages a built-in EMR script runner JAR located at:
s3://<region>.elasticmapreduce/libs/script-runner/script-runner.jar
•	This ingest.sh script is responsible for downloading real estate data from Redfin using wget, unzipping the file, and storing it in the raw data zone of your S3 bucket.

wget -O - https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz | aws s3 cp - s3://redfin-data-emr/raw-data/city_market_tracker.tsv000.gz

•	SPARK_STEPS_EXTRACTION tells EMR what to do (run a shell script): It is a list of dictionaries that describe one or more EMR steps in a format compatible with AWS EMR. In our case, it includes a single step called "Extract Redfin data".
•	EmrAddStepsOperator sends that job to the active EMR cluster
The job_flow_id is dynamically retrieved using Airflow’s XCom system, pulling from the cluster creation task’s return_value. This allows the operator to submit the Spark step to the correct EMR cluster instance. The SPARK_STEPS_EXTRACTION itself is a list of step configurations where the action is defined as CANCEL_AND_WAIT on failure, providing a safe stop in the event of step failure. Notably, AWS connection settings like aws_conn_id are not required if Airflow is already configured with IAM roles or environment-based credentials.

Adding Extraction Step
After confirming that the EMR cluster is in a ready state, the DAG proceeds to submit a Spark job using the EmrAddStepsOperator. This operator is responsible for pushing executable steps (or jobs) to the EMR cluster. But to define what the cluster actually does, we use a Python object called SPARK_STEPS_EXTRACTION.


•	After ensuring the cluster is in a ready state, the DAG submits a Spark job using the EmrAddStepsOperator. 
•	This job adds an EMR step defined in the SPARK_STEPS_EXTRACTION object. The step leverages a built-in EMR script runner JAR located at:
s3://<region>.elasticmapreduce/libs/script-runner/script-runner.jar
•	This ingest.sh script is responsible for downloading real estate data from Redfin using wget, unzipping the file, and storing it in the raw data zone of your S3 bucket.

wget -O - https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz | aws s3 cp - s3://redfin-data-emr/raw-data/city_market_tracker.tsv000.gz

•	SPARK_STEPS_EXTRACTION tells EMR what to do (run a shell script): It is a list of dictionaries that describe one or more EMR steps in a format compatible with AWS EMR. In our case, it includes a single step called "Extract Redfin data".
•	EmrAddStepsOperator sends that job to the active EMR cluster
The job_flow_id is dynamically retrieved using Airflow’s XCom system, pulling from the cluster creation task’s return_value. This allows the operator to submit the Spark step to the correct EMR cluster instance. The SPARK_STEPS_EXTRACTION itself is a list of step configurations where the action is defined as CANCEL_AND_WAIT on failure, providing a safe stop in the event of step failure. Notably, AWS connection settings like aws_conn_id are not required if Airflow is already configured with IAM roles or environment-based credentials.
