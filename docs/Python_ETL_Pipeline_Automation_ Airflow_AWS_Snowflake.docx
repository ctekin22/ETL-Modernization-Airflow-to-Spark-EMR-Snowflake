CLOUD-NATIVE PYTHON-BASED ETL PIPELINE on AWS with AIRFLOW ORCHESTRATION and SNOWFLAKE 

In this project, the goal is to build an end-to-end data pipeline that extracts real estate data from the Redfin data source and processes it entirely on the AWS cloud. The pipeline begins by provisioning an EC2 instance using an Ubuntu AMI, where Apache Airflow is installed to orchestrate and automate the workflow. The raw data is first extracted from Redfin and stored in an Amazon S3 bucket. Using Python, the data is then cleaned and transformed before being saved to a separate S3 bucket. Once the transformed data lands in S3, it automatically triggers Snowpipe, a feature in Snowflake that loads the data into a Snowflake table in near real-time. Finally, Power BI is connected to the Snowflake database for data visualization.

1.	Extract real estate data from Redfin's public datasets.
2.	Load the raw data into an Amazon S3 bucket.
3.	Transform the data using Python and Pandas.
4.	Store the cleaned data back in S3 in Parquet format.
5.	Trigger Snowflake Snowpipe automatically once the transformed data lands in S3.
6.	Load the data into a Snowflake table.
7.	Visualize it in Power BI connected to Snowflake

1.	Building the Apache Airflow DAG and Implement Extraction & Transformation Tasks

We begin this step by defining the DAG configuration parameters and implementing the first two PythonOperator tasks in Airflow — one for extracting data from Redfin and another for transforming it.

Source:
https://www.redfin.com/news/data-center/ 

https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

Refer to scripts/dags/redfin_analytics.ipynb for implementation 

We'll specify the DAG's configuration, including:
•	Start date: datetime(2025, 10, 2)
•	Email on failure/retry: Set to False for now, but can be configured to send notifications
•	Schedule interval: Since Redfin updates its data monthly (typically during the third full week), we recommend scheduling the DAG to run weekly or monthly depending on use-case.

1.2.	 Extract Redfin Data Task- extract_redfin_data
We define a PythonOperator task called extract_redfin_data task, which downloads city-level Redfin housing data and saves it to the EC2 instance with a timestamp-based filename.

1.3.	 Transform the Data Task- transform_redfin_data
Once the raw data is extracted, we define a second PythonOperator transform_redfin_data task that reads the CSV, performs transformations using Pandas, and saves the cleaned data back into S3 in Parquet format.
Initial Data Exploration, Cleanup:
•	Count total rows to understand dataset size (approx. 60 million rows).
•	Detect and count null values in each column using PySpark functions.
•	Drop rows with any null values (dropna()), resulting in a cleaner dataset
•	Extract year and month from the period_end date column as new separate columns.
•	Drop the original period_end and last_updated columns to reduce redundancy.
•	Map numeric month values (1-12) to their corresponding month names (January, February, etc.) for better interpretability.
