CLOUD-NATIVE PYTHON-BASED ETL PIPELINE on AWS with AIRFLOW ORCHESTRATION and SNOWFLAKE 

In this project, the goal is to build an end-to-end data pipeline that extracts real estate data from the Redfin data source and processes it entirely on the AWS cloud. The pipeline begins by provisioning an EC2 instance using an Ubuntu AMI, where Apache Airflow is installed to orchestrate and automate the workflow. The raw data is first extracted from Redfin and stored in an Amazon S3 bucket. Using Python, the data is then cleaned and transformed before being saved to a separate S3 bucket. Once the transformed data lands in S3, it automatically triggers Snowpipe, a feature in Snowflake that loads the data into a Snowflake table in near real-time. Finally, Power BI is connected to the Snowflake database for data visualization.

1.	Extract real estate data from Redfin's public datasets.
2.	Load the raw data into an Amazon S3 bucket.
3.	Transform the data using Python and Pandas.
4.	Store the cleaned data back in S3 in Parquet format.
5.	Trigger Snowflake Snowpipe automatically once the transformed data lands in S3.
6.	Load the data into a Snowflake table.
7.	Visualize it in Power BI connected to Snowflake

1.	Building the Apache Airflow DAG and Implement Extraction & Transformation Tasks

We begin this step by defining the DAG configuration parameters and implementing the first two PythonOperator tasks in Airflow — one for extracting data from Redfin and another for transforming it.

Source:
https://www.redfin.com/news/data-center/ 

https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

Refer to scripts/dags/redfin_analytics.ipynb for implementation 

We'll specify the DAG's configuration, including:
•	Start date: datetime(2025, 10, 2)
•	Email on failure/retry: Set to False for now, but can be configured to send notifications
•	Schedule interval: Since Redfin updates its data monthly (typically during the third full week), we recommend scheduling the DAG to run weekly or monthly depending on use-case.

1.2.	 Extract Redfin Data Task- extract_redfin_data
We define a PythonOperator task called extract_redfin_data task, which downloads city-level Redfin housing data and saves it to the EC2 instance with a timestamp-based filename.

1.3.	 Transform the Data Task- transform_redfin_data
Once the raw data is extracted, we define a second PythonOperator transform_redfin_data task that reads the CSV, performs transformations using Pandas, and saves the cleaned data back into S3 in Parquet format.
Initial Data Exploration, Cleanup:
•	Count total rows to understand dataset size (approx. 60 million rows).
•	Detect and count null values in each column using PySpark functions.
•	Drop rows with any null values (dropna()), resulting in a cleaner dataset
•	Extract year and month from the period_end date column as new separate columns.
•	Drop the original period_end and last_updated columns to reduce redundancy.
•	Map numeric month values (1-12) to their corresponding month names (January, February, etc.) for better interpretability.


1.4.	 Load Raw Extracted Data to S3 using BashOperator Task - load_to_s3
The raw CSV file was saved locally in EC2 during the extraction step. We want to preserve the original dataset in an S3 bucket. This way, you have: Transformed data in one S3 bucket for analytics. Raw data in another for traceability or reprocessing.

•	Save the cleaned and transformed DataFrame as a Parquet file to a dedicated AWS S3 bucket.
•	Use overwrite mode to ensure that the output replaces any existing data.
•	Confirm via the AWS S3 Console that the Parquet file was successfully written.

2.	Load Transformed Redfin Data into Snowflake via Snowpipe (with Airflow Orchestration)
Goal is to automate the process of loading transformed CSV data from your S3 bucket into Snowflake every time Airflow drops a new file into S3, Snowflake ingests it without manual intervention.

Component	Role
S3	Stores your cleaned/transformed CSV files (Airflow uploads them here)
Snowflake	Data warehouse where you’ll query, analyze, and visualize data
Snowpipe	A Snowflake service that automatically detects and ingests new files from S3 into a target table
Airflow (on EC2)	Still orchestrating the ETL process — uploads transformed data to S3

•	Creating Snowflake resources: database, schema, and table
•	Configuring Snowpipe to auto-load data from S3
•	Connecting S3 event notifications to trigger Snowpipe
•	Testing the pipeline end-to-end
•	Visualizing Snowflake data with BI tools

2.1.	 Create Database, Schema, and Table in Snowflake
Snowflake needs to know where to put the data (database + schema + table). You can’t load CSVs without predefining the structure. Snowflake needs to understand the structure and rules of your files (CSV, JSON, etc.) to correctly parse and ingest them.

Snowflake needs compute power to execute queries. This is done using virtual warehouses.
•	Create your table matching the transformed CSV structure

Refer to scripts/snowflake/redfin_snowflake.sql for implementation

2.2.	 Create and Configure Snowpipe to Auto-Load from S3
Snowpipe is a service that: Watches a specified S3 location. When a new file appears, it automatically runs a COPY INTO command to insert the data into a table. Without Snowpipe, you'd need to manually run SQL to load data every time Airflow uploads a file. With Snowpipe, this becomes real-time and event-driven.

•	Create STAGE in Snowflake pointing to S3 bucket. A stage is a reference to an external data location. In this case, you want Snowflake to know:
o	Where to look (your S3 bucket)
o	How to authenticate (AWS keys or IAM role)
o	How the data is structured (via file format object)
o	Use/Create an IAM role in AWS that:
	Allows Snowflake to assume the role
	Grants read access to the specific S3 path (s3://redfin-transformed-data-bucket-airflow/)
•	Create PIPE (Snowpipe) to ingest data. Just like with file formats and stages, you keep Snowpipe definitions in their own schema. This separation is essential in larger pipelines with dozens of pipes. Manually running COPY INTO every time a file lands in S3 is error-prone and slow. Snowpipe automates this. When a new file is detected in the stage:
•	It reads the file
•	Uses the file format definition
•	Runs the COPY INTO command behind the scenes
•	Inserts rows into your target table within seconds or minutes
•	Snowpipe runs in micro-batches, not streaming
•	It's event-driven if you connect S3 notifications (see next step)
•	Can be triggered by manual call, S3 event notification, or REST API
•	You configure Snowpipe with:
•	The target table 
•	The source stage 
•	The file format 
Snowpipe works with Amazon SQS, which listens for S3 events. Snowflake generates an SQS-compatible ARN when the pipe is created.
•	Run DESCRIBE PIPE your_pipe_name
•	Extract the NOTIFICATION_CHANNEL value (an SQS ARN)
•	You'll use this when setting up the event in the S3 bucket

2.3.	  Set Up S3 Event Notifications to Trigger Snowpipe
You need to tell S3 to notify Snowflake (via SQS) whenever a new object is added to the bucket or folder where transformed data land.
•	Go to your S3 bucket’s Properties > Event Notification
•	Create a new event:
o	Name: e.g., snowpipe_trigger_redfin
o	Event Type: All object create events (PUT, POST, etc.)
o	Destination: SQS (paste the SQS ARN from DESCRIBE PIPE)
•	Snowpipe will then auto-ingest the file without delay.

2.4.	 Trigger Airflow DAG to Simulate Full Pipeline
As soon as upload completes:
•	S3 triggers the event
•	Event hits the SQS queue
•	Snowpipe is notified and runs
•	COPY INTO is executed automatically
•	Data appears in Snowflake table

PART 3: Same Pipeline with PySpark on Amazon EMR: tomate EMR Jobs with Airflow

3.	Provisioning EMR and Redfin Extraction
3.1. Create an S3 bucket (e.g., redfin-emr) and folders:
•	raw_data_ – raw Redfin data
•	transformed_data – transformed output
•	scripts – PySpark or shell scripts for EMR
•	emr-logs – store EMR logs

